{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dfefa1",
   "metadata": {},
   "source": [
    "# Customer complaints analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e9033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Importing python libraries to be used in the analysis process\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import numpy\n",
    "import matplotlib\n",
    "!python -m spacy download en_core_web_sm -qq\n",
    "import emoji\n",
    "import time\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f4a69",
   "metadata": {},
   "source": [
    "### Importing customer complaints csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f337a3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'comcast_consumeraffairs_complaints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37668\\2906369978.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcustomer_complaints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"comcast_consumeraffairs_complaints\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'comcast_consumeraffairs_complaints'"
     ]
    }
   ],
   "source": [
    "customer_complaints = pd.read_csv(\"comcast_consumeraffairs_complaints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first five rows of the imported csv file\n",
    "\n",
    "customer_complaints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054770ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the complaints data to display only the elements that contain a complaint and removing the ones that are empty or null\n",
    "\n",
    "customer_complaints = customer_complaints[pd.notnull(customer_complaints['text'])]\n",
    "\n",
    "\n",
    "# Converting the 'text' column (containing text for customer complaints) to a list \n",
    "\n",
    "complaints_text_list = customer_complaints.text.values.tolist()\n",
    "\n",
    "\n",
    "# Printing individual customer's complaints on a separate line \n",
    "iterator = 0\n",
    "while len(complaints_text_list) > iterator:\n",
    "    for individual_complaint in complaints_text_list:\n",
    "    \n",
    "    # Printing the complaint that previously contained an emoji, which emoji was removed as well as the complaint after the emoji was removed.\n",
    "    \n",
    "        if len(emoji.emoji_list(individual_complaint)) > 0:\n",
    "            print(emoji.emoji_list(individual_complaint))\n",
    "            print(emoji.replace_emoji(individual_complaint, replace=\"\"))\n",
    "            complaints_text_list[iterator] =  emoji.replace_emoji(individual_complaint, replace=\"\")\n",
    "            print(\" \")\n",
    "            print(\" \")\n",
    "            print(\"The list has been upadated.\")\n",
    "            print(\" \")\n",
    "            print(iterator, \": \", complaints_text_list[iterator])\n",
    "            print(\" \")\n",
    "     \n",
    "        iterator += 1\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8660da",
   "metadata": {},
   "source": [
    "### Preprocessing complaints text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48366436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading nlp pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    " \n",
    "\n",
    "# Defining a lemmatizer function\n",
    "\n",
    "def lemmatizer(list):\n",
    "    '''\n",
    "    Lemmatize a list of texts\n",
    "    '''\n",
    "    lemmatized_list = []\n",
    "\n",
    "    doc_w_lemma = [x.lemma_ for x in list]\n",
    "\n",
    "    return doc_w_lemma\n",
    "\n",
    "# Defining a text_preprocessor function that uses the lemmatizer function for lemmatization\n",
    "\n",
    "def text_preprocessor(list):\n",
    "    '''\n",
    "    Preprocess a list of texts, removing punctuations, stopwords and lemmatizing the texts\n",
    "    '''\n",
    "    processed_list_of_complaints = []\n",
    "    \n",
    "    for x in list:\n",
    "        x = x.lower()  # To convert all text to lowercase\n",
    "        x = x.replace(\"!\", \" \")\n",
    "        x = x.replace(\".\", \" \")\n",
    "        x = x.replace(\"?\", \" \") # To remove punctuation that is connected with words like \"to?Do\" etc.\n",
    "\n",
    "        document = nlp(x) # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "        doc_no_punct = [token for token in document if token.is_punct == False ] # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "        doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]  # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "        doc_not_empty = [token for token in doc_no_stopwords if len(token) != 0]\n",
    "        lemmatized_doc = lemmatizer(doc_not_empty)\n",
    "        processed_list_of_complaints.append(lemmatized_doc)\n",
    "  \n",
    "    \n",
    "    return processed_list_of_complaints\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "list_tester = [\"Terrible service!Will never go for comcast.\", \"Worth only $10?\", \"Going to throw out my subscriptions!\", \"Had the worst experience with comcast.\", \"Expensive and not good.\", \"Coverage is very low.\", \"frequently suffers from network disconnection.\", \"I have started looking for other service companies.\", \"Not recommended for anyone looking for a stable connection.\", \"Bought it last week, Not at all satisfied.\", \"new to the service and already have complaints!\", \"Customer service is not at all cooperative.\", \"Has been weeks since last complaint was made still my complaint has not been addressed.\", \"Worst service not to be recommended to anyone.\", \"Money wasted for such an expensive service.\", \"Insufficient cabling\", \"Bad customer support\", \"No one picks up at call centre\", \"Looking for alternative better services\", \"Totally unsatisfied\", \"All claims made at the time of buying service not fulfilled\", \"Not good\", \"Will not recommend to anyone planning to work online\", \"was not that bad in the beginning, but now is the worst!\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "processed_text = text_preprocessor(list_tester)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "time_diff = end_time - start_time\n",
    "\n",
    "print(\"Time taken :  \", time_diff)\n",
    "print(\"Processed text:  Complete!\")\n",
    "print(processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to use pandas dataframe.\n",
    "\n",
    "#def text_preprocessor_text_in(text):\n",
    "#    '''\n",
    "#    Preprocess a list of texts, removing punctuations, stopwords and lemmatizing the texts\n",
    "#    '''\n",
    "#    processed_list_of_complaints = []\n",
    "    \n",
    "        \n",
    "#    for x in text:\n",
    "#        x = x.lower()  # To convert all text to lowercase\n",
    "#        x = x.replace(\"!\", \" \") # To remove punctuation that is connected with words like \"to?Do\" etc.\n",
    "#        x = x.replace(\".\", \" \")\n",
    "#        x = x.replace(\"?\", \" \")\n",
    "#        document = nlp(x) # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "#        doc_no_punct = [token for token in document if token.is_punct == False ] # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "#        doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]  # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "#        doc_not_empty = [token for token in doc_no_stopwords if len(token) != 0]\n",
    "#        lemmatized_doc = lemmatizer(doc_not_empty)\n",
    "#        processed_list_of_complaints.append(lemmatized_doc)\n",
    "\n",
    "    \n",
    "#    return processed_list_of_complaints\n",
    "\n",
    "def text_preprocessor_text_in(x):\n",
    "\n",
    "    '''\n",
    "    Preprocess a list of texts, removing punctuations, stopwords and lemmatizing the texts\n",
    "    '''\n",
    "    processed_list_of_complaints = []\n",
    "    x = x.lower()  # To convert all text to lowercase\n",
    "    x = x.replace(\"!\", \" \") # To remove punctuation that is connected with words like \"to?Do\" etc.\n",
    "    x = x.replace(\".\", \" \")\n",
    "    x = x.replace(\"?\", \" \")\n",
    "\n",
    "    document = nlp(x) # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "    doc_no_punct = [token for token in document if token.is_punct == False ] # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "    doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]  # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "    doc_not_empty = [token for token in doc_no_stopwords if len(token) != 0]\n",
    "    lemmatized_doc = lemmatizer(doc_not_empty)\n",
    "    processed_list_of_complaints.append(lemmatized_doc)\n",
    "\n",
    "    return processed_list_of_complaints\n",
    "\n",
    "st_time = time.time()\n",
    "customer_complaints['cleaned'] = customer_complaints['text'].apply(lambda x: text_preprocessor_text_in(x))\n",
    "\n",
    "en_time = time.time()\n",
    "time_taken = en_time - st_time\n",
    "print(\"Time taken :  \", time_taken)\n",
    "print(\"Processed text:\\n\")\n",
    "customer_complaints.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a single list from the processed_text list of lists\n",
    "\n",
    "processed_text_single_list_of_words = list(itertools.chain(*processed_text))\n",
    "print(processed_text_single_list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4505075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to keep track of word occurences\n",
    "def word_occurence(list):\n",
    "    counts = dict()\n",
    "    words = list\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "print( word_occurence(processed_text_single_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e8190e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1116\\2823651183.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msimple_preprocess\u001b[1;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m    309\u001b[0m     tokens = [\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     ]\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m    260\u001b[0m     \u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mto_lower\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(list_tester, deacc=False, min_len=2, max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0fb62cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
