{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dfefa1",
   "metadata": {},
   "source": [
    "# Customer complaints analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e9033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Importing python libraries to be used in the analysis process\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import numpy\n",
    "import matplotlib\n",
    "!python -m spacy download en_core_web_sm -qq\n",
    "import emoji\n",
    "import time\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f4a69",
   "metadata": {},
   "source": [
    "### Importing customer complaints csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f337a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_complaints = pd.read_csv(\"comcast_consumeraffairs_complaints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93edca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>posted_on</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alantae of Chesterfeild, MI</td>\n",
       "      <td>Nov. 22, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>I used to love Comcast. Until all these consta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vera of Philadelphia, PA</td>\n",
       "      <td>Nov. 19, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm so over Comcast! The worst internet provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sarah of Rancho Cordova, CA</td>\n",
       "      <td>Nov. 17, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>If I could give them a negative star or no sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dennis of Manchester, NH</td>\n",
       "      <td>Nov. 16, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>I've had the worst experiences so far since in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ryan of Bellevue, WA</td>\n",
       "      <td>Nov. 14, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Check your contract when you sign up for Comca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        author      posted_on  rating  \\\n",
       "0  Alantae of Chesterfeild, MI  Nov. 22, 2016       1   \n",
       "1     Vera of Philadelphia, PA  Nov. 19, 2016       1   \n",
       "2  Sarah of Rancho Cordova, CA  Nov. 17, 2016       1   \n",
       "3     Dennis of Manchester, NH  Nov. 16, 2016       1   \n",
       "4         Ryan of Bellevue, WA  Nov. 14, 2016       1   \n",
       "\n",
       "                                                text  \n",
       "0  I used to love Comcast. Until all these consta...  \n",
       "1  I'm so over Comcast! The worst internet provid...  \n",
       "2  If I could give them a negative star or no sta...  \n",
       "3  I've had the worst experiences so far since in...  \n",
       "4  Check your contract when you sign up for Comca...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first five rows of the imported csv file\n",
    "\n",
    "customer_complaints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054770ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'match_start': 2022, 'match_end': 2023, 'emoji': 'âœŒ'}]\n",
      "First off, I've been without internet for a week and on demand for a week as well. So I call the second day after nothing worked the first day to schedule a visit and the automated system kept saying that there was an outage in my area and it'll be resolved by 12:36. So I call later that afternoon and tell them and they try to send signals to the modem or whatever and it wouldn't work. So we scheduled a visit for that Friday between 10-12. Mind you that the first option they gave me was for the following freaking Monday and I said HECK NO that is way too long, I have a business to run.So the guy came on Friday at 1:30 NOT between 10-12 because I saw him pull up, he sat there for like 20 minutes and when I looked again he was freaking gone! So now I'm a little hot, so I call and ask what the heck is going on and why'd he just leave and the lady said, \"Well there's another outage in your area and he wouldn't have been able to do anything but he should have at least came to your door and told you but he didn't\". So later we rescheduled another appointment for Sunday between 3-4 and confirmed it.So I don't go to church. I'm here waiting on them to come and it's 4 and all call like, \"Where are you guys?\" And this lady on the phone tells me that it has been rescheduled for freaking Tuesday! I'm like WTF! Seriously! How the heck can you just do that?!! She said that after further checking for like 10 mins that the local office called to reschedule but no one picked up so we couldn't confirm. I was like, \"But you just said it's scheduled for Tuesday so how the heck did you confirm that if no one picked up?\" They didn't call by the way. So you just don't come?? You cancel it on your own and don't freaking show up?!! She said \"yes ma'am that's correct...\" So I'm pissed by now and then she says that someone will call me back by 6 with a rescheduled... No one has called!!! This isn't the first time something like this has happened... But I can guarantee it'll be the last... I'M GOING WITH DIRECT!!! \n",
      " \n",
      " \n",
      "The list has been upadated.\n",
      " \n",
      "36 :  First off, I've been without internet for a week and on demand for a week as well. So I call the second day after nothing worked the first day to schedule a visit and the automated system kept saying that there was an outage in my area and it'll be resolved by 12:36. So I call later that afternoon and tell them and they try to send signals to the modem or whatever and it wouldn't work. So we scheduled a visit for that Friday between 10-12. Mind you that the first option they gave me was for the following freaking Monday and I said HECK NO that is way too long, I have a business to run.So the guy came on Friday at 1:30 NOT between 10-12 because I saw him pull up, he sat there for like 20 minutes and when I looked again he was freaking gone! So now I'm a little hot, so I call and ask what the heck is going on and why'd he just leave and the lady said, \"Well there's another outage in your area and he wouldn't have been able to do anything but he should have at least came to your door and told you but he didn't\". So later we rescheduled another appointment for Sunday between 3-4 and confirmed it.So I don't go to church. I'm here waiting on them to come and it's 4 and all call like, \"Where are you guys?\" And this lady on the phone tells me that it has been rescheduled for freaking Tuesday! I'm like WTF! Seriously! How the heck can you just do that?!! She said that after further checking for like 10 mins that the local office called to reschedule but no one picked up so we couldn't confirm. I was like, \"But you just said it's scheduled for Tuesday so how the heck did you confirm that if no one picked up?\" They didn't call by the way. So you just don't come?? You cancel it on your own and don't freaking show up?!! She said \"yes ma'am that's correct...\" So I'm pissed by now and then she says that someone will call me back by 6 with a rescheduled... No one has called!!! This isn't the first time something like this has happened... But I can guarantee it'll be the last... I'M GOING WITH DIRECT!!! \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Filtering the complaints data to display only the elements that contain a complaint and removing the ones that are empty or null\n",
    "\n",
    "customer_complaints = customer_complaints[pd.notnull(customer_complaints['text'])]\n",
    "\n",
    "\n",
    "# Converting the 'text' column (containing text for customer complaints) to a list \n",
    "\n",
    "complaints_text_list = customer_complaints.text.values.tolist()\n",
    "\n",
    "\n",
    "# Printing individual customer's complaints on a separate line \n",
    "iterator = 0\n",
    "while len(complaints_text_list) > iterator:\n",
    "    for individual_complaint in complaints_text_list:\n",
    "    \n",
    "    # Printing the complaint that previously contained an emoji, which emoji was removed as well as the complaint after the emoji was removed.\n",
    "    \n",
    "        if len(emoji.emoji_list(individual_complaint)) > 0:\n",
    "            print(emoji.emoji_list(individual_complaint))\n",
    "            print(emoji.replace_emoji(individual_complaint, replace=\"\"))\n",
    "            complaints_text_list[iterator] =  emoji.replace_emoji(individual_complaint, replace=\"\")\n",
    "            print(\" \")\n",
    "            print(\" \")\n",
    "            print(\"The list has been upadated.\")\n",
    "            print(\" \")\n",
    "            print(iterator, \": \", complaints_text_list[iterator])\n",
    "            print(\" \")\n",
    "     \n",
    "        iterator += 1\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8660da",
   "metadata": {},
   "source": [
    "### Preprocessing complaints text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27e88362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  [['terrible', 'service', 'comcast'], ['terrible', 'service', 'comcast'], ['terrible', 'service', 'comcast'], ['terrible', 'service', 'comcast']]\n",
      "2  :  [['worth', '$', '10'], ['worth', '$', '10'], ['worth', '$', '10'], ['worth', '$', '10']]\n",
      "3  :  [['go', 'throw', 'subscription'], ['go', 'throw', 'subscription'], ['go', 'throw', 'subscription'], ['go', 'throw', 'subscription']]\n",
      "4  :  [['bad', 'experience', 'comcast'], ['bad', 'experience', 'comcast'], ['bad', 'experience', 'comcast'], ['bad', 'experience', 'comcast']]\n",
      "5  :  [['expensive', 'good'], ['expensive', 'good'], ['expensive', 'good']]\n",
      "6  :  [['coverage', 'low'], ['coverage', 'low'], ['coverage', 'low']]\n",
      "7  :  [['frequently', 'suffer', 'network', 'disconnection'], ['frequently', 'suffer', 'network', 'disconnection'], ['frequently', 'suffer', 'network', 'disconnection'], ['frequently', 'suffer', 'network', 'disconnection'], ['frequently', 'suffer', 'network', 'disconnection']]\n",
      "8  :  [['start', 'look', 'service', 'company'], ['start', 'look', 'service', 'company'], ['start', 'look', 'service', 'company'], ['start', 'look', 'service', 'company'], ['start', 'look', 'service', 'company']]\n",
      "9  :  [['recommend', 'look', 'stable', 'connection'], ['recommend', 'look', 'stable', 'connection'], ['recommend', 'look', 'stable', 'connection'], ['recommend', 'look', 'stable', 'connection'], ['recommend', 'look', 'stable', 'connection']]\n",
      "10  :  [['buy', 'week', 'satisfied', ' '], ['buy', 'week', 'satisfied', ' '], ['buy', 'week', 'satisfied', ' '], ['buy', 'week', 'satisfied', ' '], ['buy', 'week', 'satisfied', ' ']]\n",
      "Time taken :   7.245638370513916\n"
     ]
    }
   ],
   "source": [
    "# Loading nlp pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Defining a lemmatizer function\n",
    "\n",
    "def lemmatizer(list):\n",
    "    lemmatized_list = []\n",
    "    iterator = 0\n",
    "    \n",
    "    while len(list) >= iterator:\n",
    "        doc_w_lemma = [x.lemma_ for x in list]\n",
    "        lemmatized_list.append(doc_w_lemma)       \n",
    "        iterator += 1\n",
    "    #print(lemmatized_list[0:11])\n",
    "    return lemmatized_list\n",
    "\n",
    "def text_preprocessor(list):\n",
    "    processed_list_of_complaints = []\n",
    "    index = 0\n",
    "    while len(list) >= index:\n",
    "        \n",
    "        for x in list:\n",
    "            x = x.lower()  # To convert all text to lowercase\n",
    "            x = x.replace(\"!\", \" \").replace(\".\", \" \").replace(\"?\", \" \") # To remove punctuation that is connected with words like \"to?Do\" etc.\n",
    "            document = nlp(x) # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "            doc_no_punct = [token for token in document if token.is_punct == False ] # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "            doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]  # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "            lemmatized_doc = lemmatizer(doc_no_stopwords)\n",
    "            processed_list_of_complaints.append(lemmatized_doc)\n",
    "            # n_items = take(10, processed_dictionary_of_complaints.items())\n",
    "            # print(n_items)\n",
    "            \n",
    "        if index < 10:\n",
    "            print(index+1, \" : \", processed_list_of_complaints[index])\n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "    \n",
    "\n",
    "list_tester = [\"Terrible service!Will never go for comcast.\", \"Worth only $10?\", \"Going to throw out my subscriptions!\", \"Had the worst experience with comcast.\", \"Expensive and not good.\", \"Coverage is very low.\", \"frequently suffers from network disconnection.\", \"I have started looking for other service companies.\", \"Not recommended for anyone looking for a stable connection.\", \"Bought it last week, Not at all satisfied. \", \"new to the service and already have complaints!\", \"Customer service is not at all cooperative.\", \"Has been weeks since last complaint was made still my complaint has not been addressed.\", \"Worst service not to be recommended to anyone.\", \"Money wasted for such an expensive service.\", \"Insufficient cabling\", \"Bad customer support\", \"No one picks up at call centre\", \"Looking for alternative better services\", \"Totally unsatisfied\", \"All claims made at the time of buying service not fulfilled\", \"Not good\", \"Will not recommend to anyone planning to work online\", \"was not that bad in the beginning, but now is the worst!\"]\n",
    "my_list = ['I went jogging', 'I tried', 'Throughly intestigate the case. It is sophisticated.']\n",
    "start_time = time.time()\n",
    "text_preprocessor(list_tester)\n",
    "end_time = time.time()\n",
    "time_diff = end_time - start_time\n",
    "print(\"Time taken :  \", time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########   -- Code that works --  ###########\n",
    "# # # Setting the variable 'index' to 0 which will later on be incremented\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Using the while loop to iterate through the complaints_text_list\n",
    "\n",
    "while len(complaints_text_list) > index:\n",
    "\n",
    "    for individual_complaint in complaints_text_list:\n",
    "        \n",
    "        individual_complaint = individual_complaint.lower()   # To convert all text to lowercase\n",
    "       \n",
    "        document = nlp(individual_complaint)   # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "         \n",
    "        doc_no_punct = [token for token in document if not token.is_punct ]   # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "        doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]   # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "        #lemmatized_doc = [token.lemma_ for token in doc_no_stopwords]  # To lemmatize text like walking, resetting etc. but not working\n",
    "        if index < 10:\n",
    "            print(\"Tokens_of_complaint_\",  index + 1 , \":    \", [token.text_ for token in doc_no_stopwords])   # Printing the tokens of each complaint as a list\n",
    "            print(\" \")\n",
    "            print(\" \")\n",
    "        index += 1   # Incrementing index by 1\n",
    "\n",
    "        \n",
    "        \n",
    "#########     --- code ends ---      #########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48366436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken :   0.1463613510131836\n",
      "Processed text:\n",
      " [[['terrible', 'service', 'comcast']], [['worth', '$', '10']], [['go', 'throw', 'subscription']], [['bad', 'experience', 'comcast']], [['expensive', 'good']], [['coverage', 'low']], [['frequently', 'suffer', 'network', 'disconnection']], [['start', 'look', 'service', 'company']], [['recommend', 'look', 'stable', 'connection']], [['buy', 'week', 'satisfied', ' ']], [['new', 'service', 'complaint']], [['customer', 'service', 'cooperative']], [['week', 'complaint', 'complaint', 'address']], [['bad', 'service', 'recommend']], [['money', 'waste', 'expensive', 'service']], [['insufficient', 'cabling']], [['bad', 'customer', 'support']], [['pick', 'centre']], [['look', 'alternative', 'well', 'service']], [['totally', 'unsatisfie']], [['claim', 'time', 'buying', 'service', 'fulfil']], [['good']], [['recommend', 'plan', 'work', 'online']], [['bad', 'beginning', 'bad']]]\n"
     ]
    }
   ],
   "source": [
    "# Loading nlp pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    " \n",
    "\n",
    "# Defining a lemmatizer function\n",
    "\n",
    " \n",
    "\n",
    "def lemmatizer(list):\n",
    "    '''\n",
    "    Lemmatize a list of texts\n",
    "    '''\n",
    "    lemmatized_list = []\n",
    "    #iterator = 0\n",
    "    \n",
    "    #while len(list) >= iterator:\n",
    "    doc_w_lemma = [x.lemma_ for x in list]\n",
    "    lemmatized_list.append(doc_w_lemma)       \n",
    "    #iterator += 1\n",
    "    #print(lemmatized_list[0:11])\n",
    "    return lemmatized_list\n",
    "\n",
    " \n",
    "\n",
    "def text_preprocessor(list):\n",
    "    '''\n",
    "    Preprocess a list of texts, removing punctuations, stopwords and lemmatizing the texts\n",
    "    '''\n",
    "    processed_list_of_complaints = []\n",
    "    #index = 0\n",
    "    #while len(list) >= index:\n",
    "        \n",
    "    for x in list:\n",
    "        x = x.lower()  # To convert all text to lowercase\n",
    "        x = x.replace(\"!\", \" \")\n",
    "        x = x.replace(\".\", \" \")\n",
    "        x = x.replace(\"?\", \" \") # To remove punctuation that is connected with words like \"to?Do\" etc.\n",
    "\n",
    "        document = nlp(x) # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "        doc_no_punct = [token for token in document if token.is_punct == False ] # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "        doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]  # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "        lemmatized_doc = lemmatizer(doc_no_stopwords)\n",
    "        processed_list_of_complaints.append(lemmatized_doc)\n",
    "            # n_items = take(10, processed_dictionary_of_complaints.items())\n",
    "            # print(n_items)\n",
    "            \n",
    "        #if index < 10:\n",
    "        #    print(index+1, \" : \", processed_list_of_complaints[index])\n",
    "        \n",
    "        #index += 1\n",
    "    \n",
    "    return processed_list_of_complaints\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "list_tester = [\"Terrible service!Will never go for comcast.\", \"Worth only $10?\", \"Going to throw out my subscriptions!\", \"Had the worst experience with comcast.\", \"Expensive and not good.\", \"Coverage is very low.\", \"frequently suffers from network disconnection.\", \"I have started looking for other service companies.\", \"Not recommended for anyone looking for a stable connection.\", \"Bought it last week, Not at all satisfied. \", \"new to the service and already have complaints!\", \"Customer service is not at all cooperative.\", \"Has been weeks since last complaint was made still my complaint has not been addressed.\", \"Worst service not to be recommended to anyone.\", \"Money wasted for such an expensive service.\", \"Insufficient cabling\", \"Bad customer support\", \"No one picks up at call centre\", \"Looking for alternative better services\", \"Totally unsatisfied\", \"All claims made at the time of buying service not fulfilled\", \"Not good\", \"Will not recommend to anyone planning to work online\", \"was not that bad in the beginning, but now is the worst!\"]\n",
    "my_list = ['I went jogging', 'I tried', 'Throughly intestigate the case. It is sophisticated.']\n",
    "start_time = time.time()\n",
    "processed_text = text_preprocessor(list_tester)\n",
    "end_time = time.time()\n",
    "time_diff = end_time - start_time\n",
    "print(\"Time taken :  \", time_diff)\n",
    "print(\"Processed text:\\n\", processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cea6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggestion: consider using pandas dataframe. For example, something like this...\n",
    "\n",
    "def text_preprocessor_text_in(text):\n",
    "    '''\n",
    "    Preprocess a list of texts, removing punctuations, stopwords and lemmatizing the texts\n",
    "    '''\n",
    "    processed_list_of_complaints = []\n",
    "    #index = 0\n",
    "    #while len(list) >= index:\n",
    "        \n",
    "    for x in text:\n",
    "        x = x.lower()  # To convert all text to lowercase\n",
    "        x = x.replace(\"!\", \" \") # To remove punctuation that is connected with words like \"to?Do\" etc.\n",
    "        x = x.replace(\".\", \" \")\n",
    "        x = x.replace(\"?\", \" \")\n",
    "        document = nlp(x) # Creating a document by running individual complaints through the nlp pipeline for tokenizing\n",
    "        doc_no_punct = [token for token in document if token.is_punct == False ] # Creating a refined list of tokens from the 'document' tokens without any punctuation\n",
    "        doc_no_stopwords = [token for token in doc_no_punct if token.is_stop == False]  # Further refining list of tokens from 'doc_no_punct' tokens without any stopwords\n",
    "        lemmatized_doc = lemmatizer(doc_no_stopwords)\n",
    "        processed_list_of_complaints.append(lemmatized_doc)\n",
    "            # n_items = take(10, processed_dictionary_of_complaints.items())\n",
    "            # print(n_items)\n",
    "            \n",
    "        #if index < 10:\n",
    "        #    print(index+1, \" : \", processed_list_of_complaints[index])\n",
    "        \n",
    "        #index += 1\n",
    "    \n",
    "    return processed_list_of_complaints\n",
    "    \n",
    "st_time = time.time()\n",
    "customer_complaints['cleaned'] = customer_complaints['text'].apply(lambda x: text_preprocessor_text_in(x))\n",
    "\n",
    "en_time = time.time()\n",
    "time_taken = en_time - st_time\n",
    "print(\"Time taken :  \", time_taken)\n",
    "print(\"Processed text:\\n\")\n",
    "customer_complaints.head()\n",
    "\n",
    "#This will create a new column in your customer_complaints dataframe. This way you can have the original text ('text') and processed text ('cleaned') side-by-side in the same dataframe.\n",
    "\n",
    "#Note: you need to change the text_processor function so that it accepts a text rather than a list of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0ec76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
